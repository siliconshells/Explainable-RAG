


Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
Ramprasaath R. Selvaraju · Michael Cogswell · Abhishek Das · Ramakrishna Vedantam · Devi Parikh · Dhruv Batra

Abstract We propose a technique for producing ‘visual ex-planations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.
Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.
Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforce-ment learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visual-izations to create a high-resolution class-discriminative vi-

Ramprasaath R. Selvaraju
Georgia Institute of Technology, Atlanta, GA, USA E-mail: ramprs@gatech.edu
Michael Cogswell
Georgia Institute of Technology, Atlanta, GA, USA E-mail: cogswell@gatech.edu
Abhishek Das
Georgia Institute of Technology, Atlanta, GA, USA E-mail: abhshkdz@gatech.edu
Ramakrishna Vedantam
Georgia Institute of Technology, Atlanta, GA, USA E-mail: vrama@gatech.edu
Devi Parikh
Georgia Institute of Technology, Atlanta, GA, USA Facebook AI Research, Menlo Park, CA, USA
E-mail: parikh@gatech.edu
Dhruv Batra
Georgia Institute of Technology, Atlanta, GA, USA Facebook AI Research, Menlo Park, CA, USA
E-mail: dbatra@gatech.edu
sualization, Guided Grad-CAM, and apply it to image clas-sification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures.
In the context of image classification models, our visualiza-tions (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have rea-sonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias.
For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discrimina-tive regions of input image.
We devise a way to identify important neurons through Grad-CAM and combine it with neuron names [4] to provide tex-tual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM ex-planations help users establish appropriate trust in predic-tions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep net-work from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ ramprs/grad-cam/, along with a demo on CloudCV [2]1, and a video at youtu.be/COjUB9Izk6E.

Introduction

Deep neural models based on Convolutional Neural Net-works (CNNs) have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classi-fication [33, 24], object detection [21], semantic segmenta-
tion [37] to image captioning [55, 7, 18, 29], visual question
answering [3, 20, 42, 46] and more recently, visual dialog [11,
13, 12] and embodied question answering [10, 23]. While
 	
1   http://gradcam.cloudcv.org
these models enable superior performance, their lack of de-composability into individually intuitive components makes them hard to interpret [36]. Consequently, when today’s intel-ligent systems fail, they often fail spectacularly disgracefully without warning or explanation, leaving a user staring at an incoherent output, wondering why the system did what it did.
Interpretability matters. In order to build trust in intelligent systems and move towards their meaningful integration into our everyday lives, it is clear that we must build ‘transparent’ models that have the ability to explain why they predict what they predict. Broadly speaking, this transparency and abil-ity to explain is useful at three different stages of Artificial Intelligence (AI) evolution. First, when AI is significantly weaker than humans and not yet reliably deployable (e.g. visual question answering [3]), the goal of transparency and explanations is to identify the failure modes [1, 25], thereby helping researchers focus their efforts on the most fruitful re-search directions. Second, when AI is on par with humans and reliably deployable (e.g., image classification [30] trained on sufficient data), the goal is to establish appropriate trust and confidence in users. Third, when AI is significantly stronger than humans (e.g. chess or Go [50]), the goal of explana-tions is in machine teaching [28] – i.e., a machine teaching a human about how to make better decisions.
There typically exists a trade-off between accuracy and sim-plicity or interpretability. Classical rule-based or expert sys-tems [26] are highly interpretable but not very accurate (or robust). Decomposable pipelines where each stage is hand-designed are thought to be more interpretable as each indi-vidual component assumes a natural intuitive explanation. By using deep models, we sacrifice interpretable modules for uninterpretable ones that achieve greater performance through greater abstraction (more layers) and tighter integra-tion (end-to-end training). Recently introduced deep residual networks (ResNets) [24] are over 200-layers deep and have shown state-of-the-art performance in several challenging tasks. Such complexity makes these models hard to interpret. As such, deep models are beginning to explore the spectrum between interpretability and accuracy.
Zhou et al. [59] recently proposed a technique called Class Activation Mapping (CAM) for identifying discriminative regions used by a restricted class of image classification CNNs which do not contain any fully-connected layers. In essence, this work trades off model complexity and perfor-mance for more transparency into the working of the model. In contrast, we make existing state-of-the-art deep models interpretable without altering their architecture, thus avoiding the interpretability vs. accuracy trade-off. Our approach is a generalization of CAM [59] and is applicable to a signifi-cantly broader range of CNN model families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for struc-tured outputs (e.g. captioning), (3) CNNs used in tasks with
multi-modal inputs (e.g. VQA) or reinforcement learning, without requiring architectural changes or re-training.
What makes a good visual explanation? Consider image classification [14] – a ‘good’ visual explanation from the model for justifying any target category should be (a) class-discriminative (i.e. localize the category in the image) and
(b) high-resolution (i.e. capture fine-grained detail).
Fig. 1 shows outputs from a number of visualizations for the ‘tiger cat’ class (top) and ‘boxer’ (dog) class (bottom). Pixel-space gradient visualizations such as Guided Back-propagation [53] and Deconvolution [57] are high-resolution and highlight fine-grained details in the image, but are not class-discriminative (Fig. 1b and Fig. 1h are very similar).
In contrast, localization approaches like CAM or our pro-posed method Gradient-weighted Class Activation Mapping (Grad-CAM), are highly class-discriminative (the ‘cat’ expla-nation exclusively highlights the ‘cat’ regions but not ‘dog’ regions in Fig. 1c, and vice versa in Fig. 1i).
In order to combine the best of both worlds, we show that it is possible to fuse existing pixel-space gradient visualizations with Grad-CAM to create Guided Grad-CAM visualizations that are both high-resolution and class-discriminative. As a result, important regions of the image which correspond to any decision of interest are visualized in high-resolution de-tail even if the image contains evidence for multiple possible concepts, as shown in Figures 1d and 1j. When visualized for ‘tiger cat’, Guided Grad-CAM not only highlights the cat regions, but also highlights the stripes on the cat, which is important for predicting that particular variety of cat.
To summarize, our contributions are as follows:
We introduce Grad-CAM, a class-discriminative local-ization technique that generates visual explanations for any CNN-based network without requiring architectural changes
or re-training. We evaluate Grad-CAM for localization (Sec. 4.1), and faithfulness to model (Sec. 5.3), where it outperforms baselines.
We apply Grad-CAM to existing top-performing classi-fication, captioning (Sec. 8.1), and VQA (Sec. 8.2) models. For image classification, our visualizations lend insight into failures of current CNNs (Sec. 6.1), showing that seemingly unreasonable predictions have reasonable explanations. For captioning and VQA, our visualizations expose that common CNN + LSTM models are often surprisingly good at local-izing discriminative image regions despite not being trained on grounded image-text pairs.
We show a proof-of-concept of how interpretable Grad-CAM visualizations help in diagnosing failure modes by uncovering biases in datasets. This is important not just for generalization, but also for fair and bias-free outcomes as more and more decisions are made by algorithms in society.
We present Grad-CAM visualizations for ResNets [24] applied to image classification and VQA (Sec. 8.2).

    

Fig. 1: (a) Original image with a cat and a dog. (b-f) Support for the cat category according to various visualizations for VGG-16 and ResNet. (b) Guided Backpropagation [53]: highlights all contributing features. (c, f) Grad-CAM (Ours): localizes class-discriminative regions, (d) Combining
(b) and (c) gives Guided Grad-CAM, which gives high-resolution class-discriminative visualizations. Interestingly, the localizations achieved by our Grad-CAM technique, (c) are very similar to results from occlusion sensitivity (e), while being orders of magnitude cheaper to compute. (f, l) are Grad-CAM visualizations for ResNet-18 layer. Note that in (c, f, i, l), red regions corresponds to high score for class, while in (e, k), blue corresponds to evidence for the class. Figure best viewed in color.

We use neuron importance from Grad-CAM and neuron names from [4] and obtain textual explanations for model decisions (Sec. 7).
We conduct human studies (Sec. 5) that show Guided Grad-CAM explanations are class-discriminative and not only help humans establish trust, but also help untrained users successfully discern a ‘stronger’ network from a ‘weaker’ one, even when both make identical predictions.
Paper Organization: The rest of the paper is organized as follows. In section 3 we propose our approach Grad-CAM and Guided Grad-CAM. In sections 4 and 5 we evaluate the localization ability, class-discriminativeness, trustworthyness and faithfulness of Grad-CAM. In section 6 we show certain use cases of Grad-CAM such as diagnosing image classifi-cation CNNs and identifying biases in datasets. In section 7 we provide a way to obtain textual explanations with Grad-CAM. In section 8 we show how Grad-CAM can be applied to vision and language models – image captioning and Visual Question Answering (VQA).
Related Work

Our work draws on recent work in CNN visualizations, model trust assessment, and weakly-supervised localization.
Visualizing CNNs. A number of previous works [51, 53, 57, 19] have visualized CNN predictions by highlighting ‘impor-tant’ pixels (i.e. change in intensities of these pixels have the most impact on the prediction score). Specifically, Si-monyan et al. [51] visualize partial derivatives of predicted class scores w.r.t. pixel intensities, while Guided Backprop-agation [53] and Deconvolution [57] make modifications to ‘raw’ gradients that result in qualitative improvements. These approaches are compared in [40]. Despite produc-
ing fine-grained visualizations, these methods are not class-discriminative. Visualizations with respect to different classes are nearly identical (see Figures 1b and 1h).
Other visualization methods synthesize images to maximally activate a network unit [51, 16] or invert a latent represen-tation [41, 15]. Although these can be high-resolution and class-discriminative, they are not specific to a single input image and visualize a model overall.
Assessing Model Trust. Motivated by notions of interpretabil-ity [36] and assessing trust in models [47], we evaluate Grad-CAM visualizations in a manner similar to [47] via human studies to show that they can be important tools for users to evaluate and place trust in automated systems.
Aligning Gradient-based Importances. Selvaraju et al. [48] proposed an approach that uses the gradient-based neuron importances introduced in our work, and maps it to class-specific domain knowledge from humans in order to learn classifiers for novel classes. In future work, Selvaraju et al.
[49] proposed an approach to align gradient-based impor-tances to human attention maps in order to ground vision and language models.
Weakly-supervised localization. Another relevant line of work is weakly-supervised localization in the context of CNNs, where the task is to localize objects in images us-ing holistic image class labels only [8, 43, 44, 59].
Most relevant to our approach is the Class Activation Map-ping (CAM) approach to localization [59]. This approach modifies image classification CNN architectures replacing fully-connected layers with convolutional layers and global average pooling [34], thus achieving class-specific feature maps. Others have investigated similar methods using global max pooling [44] and log-sum-exp pooling [45].


Fig. 2: Grad-CAM overview: Given an image and a class of interest (e.g., ‘tiger cat’ or any other type of differentiable output) as input, we forward propagate the image through the CNN part of the model and then through task-specific computations to obtain a raw score for the category. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the rectified convolutional feature maps of interest, which we combine to compute the coarse Grad-CAM localization (blue heatmap) which represents where the model has to look to make the particular decision. Finally, we pointwise multiply the heatmap with guided backpropagation to get Guided Grad-CAM visualizations which are both high-resolution and concept-specific.

A drawback of CAM is that it requires feature maps to di-rectly precede softmax layers, so it is only applicable to a particular kind of CNN architectures performing global av-erage pooling over convolutional maps immediately prior to prediction (i.e. conv feature maps → global average pooling
→ softmax layer). Such architectures may achieve inferior accuracies compared to general networks on some tasks (e.g. image classification) or may simply be inapplicable to any other tasks (e.g. image captioning or VQA). We introduce a new way of combining feature maps using the gradient signal that does not require any modification in the network architec-ture. This allows our approach to be applied to off-the-shelf CNN-based architectures, including those for image caption-ing and visual question answering. For a fully-convolutional architecture, CAM is a special case of Grad-CAM.
Other methods approach localization by classifying perturba-tions of the input image. Zeiler and Fergus [57] perturb inputs
Grad-CAM

A number of previous works have asserted that deeper repre-sentations in a CNN capture higher-level visual constructs [6, 41]. Furthermore, convolutional layers naturally retain spatial information which is lost in fully-connected layers, so we can expect the last convolutional layers to have the best com-promise between high-level semantics and detailed spatial information. The neurons in these layers look for semantic class-specific information in the image (say object parts). Grad-CAM uses the gradient information flowing into the last convolutional layer of the CNN to assign importance values to each neuron for a particular decision of interest. Although our technique is fairly general in that it can be used to explain activations in any layer of a deep network, in this work, we focus on explaining output layer decisions only.
As shown in Fig. 2, in order to obtain the class-discriminative
by occluding patches and classifying the occluded image,
localization map Grad-CAM Lc
∈ Ru×v of width u
typically resulting in lower classification scores for relevant objects when those objects are occluded. This principle is ap-plied for localization in [5]. Oquab et al. [43] classify many patches containing a pixel then average these patch-wise scores to provide the pixel’s class-wise score. Unlike these, our approach achieves localization in one shot; it only re-quires a single forward and a partial backward pass per image
and thus is typically an order of magnitude more efficient. In
and height v for any class c, we first compute the gradient of the score for class c, yc (before the softmax), with respect to feature map activations Ak of a convolutional layer, i.e. ∂yc . These gradients flowing back are global-average-pooled 2 over the width and height dimensions (indexed by i and j respectively) to obtain the neuron importance weights αc :
global average pooling
recent work, Zhang et al. [58] introduce contrastive Marginal Winning Probability (c-MWP), a probabilistic Winner-Take-
All formulation for modelling the top-down attention for neural classification models which can highlight discrimina-tive regions. This is computationally more expensive than Grad-CAM and only works for image classification CNNs.
i	j	ij

gradients via backprop
During computation of αc while backpropagating gradients with respect to activations, the exact computation amounts
Moreover, Grad-CAM outperforms c-MWP in quantitative		
and qualitative evaluations (see Sec. 4.1 and Sec. D).
2 Empirically we found global-average-pooling to work better than global-max-pooling as can be found in the Appendix.
to successive matrix products of the weight matrices and the gradient with respect to activation functions till the final convolution layer that the gradients are being propagated to. Hence, this weight αc represents a partial linearization of the deep network downstream from A, and captures the ‘importance’ of feature map k for a target class c.
We perform a weighted combination of forward activation maps, and follow it by a ReLU to obtain,

class. Taking the gradient of the score for class c (Y c)
Notice that this results in a coarse heatmap of the same size as the convolutional feature maps (14 × 14 in the case of
with respect to the feature map Fk we get,
∂Y c
last convolutional layers of VGG [52] and AlexNet [33] networks) 3. We apply a ReLU to the linear combination of maps because we are only interested in the features that
∂Y c
∂Fk =
k
ij


 ∂Fk 
k ij
(6)
have a positive influence on the class of interest, i.e. pixels
Taking partial derivative of (4) w.r.t. Ak , we can see that
whose intensity should be increased in order to increase yc.	k
Negative pixels are likely to belong to other categories in the
 ∂F  =  1 . Substituting this in (6), we get,
ij
image. As expected, without this ReLU, localization maps sometimes highlight more than just the desired class and perform worse at localization. Figures 1c, 1f and 1i, 1l show Grad-CAM visualizations for ‘tiger cat’ and ‘boxer (dog)’
∂Ak	Z

∂Y c
∂Fk =


respectively. Ablation studies are available in Sec. B.
From (5) we get that, ∂Y c = wc . Hence,
In general, yc need not be the class score produced by an image classification CNN. It could be any differentiable activation including words from a caption or answer to a question.


Grad-CAM generalizes CAM

Summing both sides of (8) over all pixels (i, j),
(8)

In this section, we discuss the connections between Grad-

CAM and Class Activation Mapping (CAM) [59], and for-mally prove that Grad-CAM generalizes CAM for a wide variety of CNN-based architectures. Recall that CAM pro-duces a localization map for an image classification CNN

Since Z and wc do not depend on (i, j), rewriting this as

c
with a specific kind of architecture where global average
(10)
pooled convolutional feature maps are fed directly into soft-max. Specifically, let the penultimate layer produce K feature maps, Ak ∈ Ru×v, with each element indexed by i, j. So

Note that Z is the number of pixels in the feature map (or
k refers to the activation at location (i, j) of the feature
Z = Li Lj
1). Thus, we can re-order terms and see that
map Ak. These feature maps are then spatially pooled using Global Average Pooling (GAP) and linearly transformed to produce a score Y c for each class 

(11)
global average pooling

(3)
Up to a proportionality constant (1/Z) that gets normalized-out during visualization, the expression for wc is identical to used by Grad-CAM (1). Thus, Grad-CAM is a strict gen-
3 We find that Grad-CAM maps become progressively worse as we move to earlier convolutional layers as they have smaller receptive fields and only focus on less semantic local features.
visual explanations from CNN-based models that cascade convolutional layers with much more complex interactions, such as those for image captioning and VQA (Sec. 8.2).
Guided Grad-CAM

While Grad-CAM is class-discriminative and localizes rel-evant image regions, it lacks the ability to highlight fine-grained details like pixel-space gradient visualization meth-ods (Guided Backpropagation [53], Deconvolution [57]). Guided Backpropagation visualizes gradients with respect to the image where negative gradients are suppressed when backpropagating through ReLU layers. Intuitively, this aims to capture pixels detected by neurons, not the ones that sup-press neurons. See Figure 1c, where Grad-CAM can easily lo-calize the cat; however, it is unclear from the coarse heatmap why the network predicts this particular instance as ‘tiger cat’. In order to combine the best aspects of both, we fuse Guided Backpropagation and Grad-CAM visualizations via






(a) Original Image	(b) Cat Counterfactual exp (c) Dog Counterfactual exp

Fig. 3: Counterfactual Explanations with Grad-CAM

localization challenge [14] requires approaches to provide bounding boxes in addition to classification labels. Similar to classification, evaluation is performed for both the top-1 and top-5 predicted categories.
Given an image, we first obtain class predictions from our
element-wise multiplication (Lc
is first upsampled
network and then generate Grad-CAM maps for each of the
predicted classes and binarize them with a threshold of 15%
to the input image resolution using bilinear interpolation). Fig. 2 bottom-left illustrates this fusion. This visualization is both high-resolution (when the class of interest is ‘tiger cat’, it identifies important ‘tiger cat’ features like stripes, pointy ears and eyes) and class-discriminative (it highlights the ‘tiger cat’ but not the ‘boxer (dog)’). Replacing Guided Backpropagation with Deconvolution gives similar results, but we found Deconvolution visualizations to have artifacts and Guided Backpropagation to be generally less noisy.

Counterfactual Explanations

Using a slight modification to Grad-CAM, we can obtain explanations that highlight support for regions that would make the network change its prediction. As a consequence, removing concepts occurring in those regions would make the model more confident about its prediction. We refer to this explanation modality as counterfactual explanations.
Specifically, we negate the gradient of yc (score for class c) with respect to feature maps A of a convolutional layer. Thus the importance weights αc now become
global average pooling
of the max intensity. This results in connected segments of pixels and we draw a bounding box around the single largest segment. Note that this is weakly-supervised localization – the models were never exposed to bounding box annotations during training.
We evaluate Grad-CAM localization with off-the-shelf pre-trained VGG-16 [52] , AlexNet [33] and GoogleNet [54] (obtained from the Caffe [27] Zoo). Following ILSVRC-15 evaluation, we report both top-1 and top-5 localization errors on the val set in Table. 1. Grad-CAM localization errors are significantly better than those achieved by c-MWP [58] and Simonyan et al. [51], which use grab-cut to post-process im-age space gradients into heat maps. Grad-CAM for VGG-16 also achieves better top-1 localization error than CAM [59], which requires a change in the model architecture, necessi-tates re-training and thereby achieves worse classification errors (2.98% worse top-1), while Grad-CAM does not com-promise on classification performance.
Classification	Localization


c-MWP [58]	44.2	20.8	92.6	89.2
Grad-CAM (ours)	44.2	20.8	68.3	56.6


As in (2), we take a weighted sum of the forward activation maps, A, with weights αc , and follow it by a ReLU to obtain counterfactual explanations as shown in Fig. 3.

Evaluating Localization Ability of Grad-CAM

Weakly-supervised Localization

In this section, we evaluate the localization capability of Grad-CAM in the context of image classification. The ImageNet
Grad-CAM (ours)	31.9	11.3	60.09	49.34
CAM [59]	31.9	11.3	60.09	49.34


Table 1: Classification and localization error % on ILSVRC-15 val (lower is better) for VGG-16, AlexNet and GoogleNet. We see that Grad-CAM achieves superior localization errors without compromising on classification performance.

Weakly-supervised Segmentation

Semantic segmentation involves the task of assigning each pixel in the image an object class (or background class). Be-
ing a challenging task, this requires expensive pixel-level annotation. The task of weakly-supervised segmentation in-volves segmenting objects with just image-level annotation, which can be obtained relatively cheaply from image classi-fication datasets. In recent work, Kolesnikov et al. [32] in-troduced a new loss function for training weakly-supervised image segmentation models. Their loss function is based on three principles – 1) to seed with weak localization cues, encouraging segmentation network to match these cues, 2) to expand object seeds to regions of reasonable size based on information about which classes can occur in an image, 3) to constrain segmentations to object boundaries that alleviates the problem of imprecise boundaries already at training time. They showed that their proposed loss function, consisting of the above three losses leads to better segmentation.
However, their algorithm is sensitive to the choice of weak localization seed, without which the network fails to localize objects correctly. In their work, they used CAM maps from a VGG-16 based network which are used as object seeds for weakly localizing foreground classes. We replaced the CAM maps with Grad-CAM obtained from a standard VGG-16 network and obtain a Intersection over Union (IoU) score of
49.6 (compared to 44.6 obtained with CAM) on the PASCAL VOC 2012 segmentation task. Fig. 4 shows some qualitative results.

Fig. 4: PASCAL VOC 2012 Segmentation results with Grad-CAM as seed for SEC [32].


Pointing Game

Zhang et al. [58] introduced the Pointing Game experiment to evaluate the discriminativeness of different visualization methods for localizing target objects in scenes. Their eval-uation protocol first cues each visualization technique with the ground-truth object label and extracts the maximally acti-vated point on the generated heatmap. It then evaluates if the point lies within one of the annotated instances of the target object category, thereby counting it as a hit or a miss.
The localization accuracy is then calculated as
Acc =   #Hits   . However, this evaluation only mea-sures precision of the visualization technique. We modify the protocol to also measure recall – we compute localization
maps for top-5 class predictions from the CNN classifiers4 and evaluate them using the pointing game setup with an additional option to reject any of the top-5 predictions from the model if the maximally activated point in the map is below a threshold, i.e. if the visualization correctly rejects the predictions which are absent from the ground-truth cat-egories, it gets that as a hit. We find that Grad-CAM out-performs c-MWP [58] by a significant margin (70.58% vs. 60.30%). Qualitative examples comparing c-MWP [58] and Grad-CAM on can be found in Sec. D5.

Evaluating Visualizations

In this section, we describe the human studies and exper-iments we conducted to understand the interpretability vs. faithfulness tradeoff of our approach to model predictions. Our first human study evaluates the main premise of our approach – are Grad-CAM visualizations more class discrim-inative than previous techniques? Having established that, we turn to understanding whether it can lead an end user to trust the visualized models appropriately. For these experiments, we compare VGG-16 and AlexNet finetuned on PASCAL VOC 2007 train and visualizations evaluated on val.

Evaluating Class Discrimination

In order to measure whether Grad-CAM helps distinguish be-tween classes, we select images from the PASCAL VOC 2007 val set, which contain exactly 2 annotated categories and cre-ate visualizations for each one of them. For both VGG-16 and AlexNet CNNs, we obtain category-specific visualizations using four techniques: Deconvolution, Guided Backpropa-gation, and Grad-CAM versions of each of these methods (Deconvolution Grad-CAM and Guided Grad-CAM). We show these visualizations to 43 workers on Amazon Mechan-ical Turk (AMT) and ask them “Which of the two object categories is depicted in the image?” (shown in Fig. 5).
Intuitively, a good prediction explanation is one that pro-duces discriminative visualizations for the class of interest. The experiment was conducted using all 4 visualizations for 90 image-category pairs (i.e. 360 visualizations); 9 rat-ings were collected for each image, evaluated against the ground truth and averaged to obtain the accuracy in Table. 2. When viewing Guided Grad-CAM, human subjects can cor-rectly identify the category being visualized in 61.23% of cases (compared to 44.44% for Guided Backpropagation; thus, Grad-CAM improves human performance by 16.79%). Similarly, we also find that Grad-CAM helps make Deconvo-lution more class-discriminative (from 53.33% → 60.37%). Guided Grad-CAM performs the best among all methods.

4 We use GoogLeNet finetuned on COCO, as provided by [58].
5 c-MWP [58] highlights arbitrary regions for predicted but non-existent categories, unlike Grad-CAM maps which typically do not.

	

(a) Raw input image. Note that this is not a part of the tasks (b) and (c)
(b) AMT interface for evaluating the class-
discriminative property	(c) AMT interface for evaluating if our visualizations instill trust in an end user
Fig. 5: AMT interfaces for evaluating different visualizations for class discrimination (b) and trustworthiness (c). Guided Grad-CAM outperforms baseline approaches (Guided-backprop and Deconvolution) showing that our visualizations are more class-discriminative and help humans place trust in a more accurate classifier.

Interestingly, our results indicate that Deconvolution is more class-discriminative than Guided Backpropagation (53.33% vs. 44.44%), although Guided Backpropagation is more aes-thetically pleasing. To the best of our knowledge, our evalua-tions are the first to quantify this subtle difference.
average score of 1.00 which means that it is slightly more reliable than AlexNet, while Guided Grad-CAM achieves a higher score of 1.27 which is closer to saying that VGG-16 is clearly more reliable. Thus, our visualizations can help users place trust in a model that generalizes better, just based on

Method		Human Classification Accuracy

Relative Reli-ability

Rank Correlation  w/ Occlusion
individual prediction explanations.


Guided Backpropagation  44.44	+1.00	0.168
Guided Grad-CAM	61.23	+1.27	0.261


Table 2: Quantitative Visualization Evaluation. Guided Grad-CAM enables humans to differentiate between visualizations of different classes (Human Classification Accuracy) and pick more reliable models (Relative Reliability). It also accurately reflects the behavior of the model (Rank Correlation w/ Occlusion).


Evaluating Trust

Given two prediction explanations, we evaluate which seems more trustworthy. We use AlexNet and VGG-16 to compare Guided Backpropagation and Guided Grad-CAM visualiza-tions, noting that VGG-16 is known to be more reliable than AlexNet with an accuracy of 79.09 mAP (vs. 69.20 mAP) on PASCAL classification. In order to tease apart the efficacy of the visualization from the accuracy of the model being visu-alized, we consider only those instances where both models made the same prediction as ground truth. Given a visualiza-tion from AlexNet and one from VGG-16, and the predicted object category, 54 AMT workers were instructed to rate the reliability of the models relative to each other on a scale of clearly more/less reliable (+/-2), slightly more/less reliable (+/-1), and equally reliable (0). This interface is shown in Fig. 5. To eliminate any biases, VGG-16 and AlexNet were assigned to be ‘model-1’ with approximately equal proba-bility. Remarkably, as can be seen in Table. 2, we find that human subjects are able to identify the more accurate classi-fier (VGG-16 over AlexNet) simply from the prediction ex-planations, despite both models making identical predictions. With Guided Backpropagation, humans assign VGG-16 an
Faithfulness vs. Interpretability

Faithfulness of a visualization to a model is its ability to ac-curately explain the function learned by the model. Naturally, there exists a trade-off between the interpretability and faith-fulness of a visualization – a more faithful visualization is typically less interpretable and vice versa. In fact, one could argue that a fully faithful explanation is the entire descrip-tion of the model, which in the case of deep models is not interpretable/easy to visualize. We have verified in previous sections that our visualizations are reasonably interpretable. We now evaluate how faithful they are to the underlying model. One expectation is that our explanations should be locally accurate, i.e. in the vicinity of the input data point, our explanation should be faithful to the model [47].
For comparison, we need a reference explanation with high local-faithfulness. One obvious choice for such a visualiza-tion is image occlusion [57], where we measure the differ-ence in CNN scores when patches of the input image are masked. Interestingly, patches which change the CNN score are also patches to which Grad-CAM and Guided Grad-CAM assign high intensity, achieving rank correlation 0.254 and
0.261 (vs. 0.168, 0.220 and 0.208 achieved by Guided Back-propagation, c-MWP and CAM respectively) averaged over 2510 images in the PASCAL 2007 val set. This shows that Grad-CAM is more faithful to the original model compared to prior methods. Through localization experiments and hu-man studies, we see that Grad-CAM visualizations are more interpretable, and through correlation with occlusion maps, we see that Grad-CAM is more faithful to the model.
Diagnosing image classification CNNs with Grad-CAM

In this section we further demonstrate the use of Grad-CAM in analyzing failure modes of image classification CNNs, understanding the effect of adversarial noise, and identifying and removing biases in datasets, in the context of VGG-16 pretrained on imagenet.

Analyzing failure modes for VGG-16
category that is not present in the image and low probabilities to categories that are present. We then compute Grad-CAM visualizations for the categories that are present. As shown in Fig. 7, despite the network being certain about the absence of these categories (‘tiger cat’ and ‘boxer’), Grad-CAM vi-sualizations can correctly localize them. This shows that Grad-CAM is fairly robust to adversarial noise.














Tiger Cat: 6.5e-17
Grad-CAM “Cat”
Airliner: 0.9999
Adversarial image














Airliner: 0.9999
Grad-CAM “Airliner”
Boxer: 1.1e-20
Grad-CAM “Dog”














Space shuttle: 1e-5
Grad-CAM “Space Shuttle”




(b)	(c)	(d)
Fig. 6: In these cases the model (VGG-16) failed to predict the correct class in its top 1 (a and d) and top 5 (b and c) predictions. Humans would find it hard to explain some of these predictions without looking at the visualization for the predicted class. But with Grad-CAM, these mistakes seem justifiable.

In order to see what mistakes a network is making, we first get a list of examples that the network (VGG-16) fails to classify correctly. For these misclassified examples, we use Guided Grad-CAM to visualize both the correct and the predicted class. As seen in Fig. 6, some failures are due to ambiguities inherent in ImageNet classification. We can also see that seemingly unreasonable predictions have reasonable explanations, an observation also made in HOGgles [56]. A major advantage of Guided Grad-CAM visualizations over other methods is that due to its high-resolution and ability to be class-discriminative, it readily enables these analyses.

Effect of adversarial noise on VGG-16

Goodfellow et al. [22] demonstrated the vulnerability of cur-rent deep networks to adversarial examples, which are slight imperceptible perturbations of input images that fool the net-work into misclassifying them with high confidence. We gen-erate adversarial images for an ImageNet-pretrained VGG-16 model such that it assigns high probability (> 0.9999) to a
Fig. 7: (a-b) Original image and the generated adversarial image for category “airliner”. (c-d) Grad-CAM visualizations for the original cate-gories “tiger cat” and “boxer (dog)” along with their confidence. Despite the network being completely fooled into predicting the dominant cat-egory label of “airliner” with high confidence (>0.9999), Grad-CAM can localize the original categories accurately. (e-f) Grad-CAM for the top-2 predicted classes “airliner” and “space shuttle” seems to highlight the background.


Identifying bias in dataset

In this section, we demonstrate another use of Grad-CAM: identifying and reducing bias in training datasets. Models trained on biased datasets may not generalize to real-world scenarios, or worse, may perpetuate biases and stereotypes (w.r.t. gender, race, age, etc.). We finetune an ImageNet-pretrained VGG-16 model for a “doctor” vs. “nurse” binary classification task. We built our training and validation splits using the top 250 relevant images (for each class) from a popular image search engine. And the test set was controlled to be balanced in its distribution of genders across the two classes. Although the trained model achieves good validation accuracy, it does not generalize well (82% test accuracy).
Grad-CAM visualizations of the model predictions (see the red box6 regions in the middle column of Fig. 8) revealed that the model had learned to look at the person’s face / hairstyle to distinguish nurses from doctors, thus learning

6 The green and red boxes are drawn manually to highlight correct and incorrect focus of the model.
a gender stereotype. Indeed, the model was misclassifying several female doctors to be a nurse and male nurses to be a doctor. Clearly, this is problematic. Turns out the image search results were gender-biased (78% of images for doctors were men, and 93% images for nurses were women).

Fig. 8: In the first row, we can see that even though both models made the right decision, the biased model (model1) was looking at the face of the person to decide if the person was a nurse, whereas the unbiased model was looking at the short sleeves to make the decision. For the example image in the second row, the biased model made the wrong prediction (misclassifying a doctor as a nurse) by looking at the face and the hairstyle, whereas the unbiased model made the right prediction looking at the white coat, and the stethoscope.

Through these intuitions gained from Grad-CAM visualiza-tions, we reduced bias in the training set by adding in images of male nurses and female doctors, while maintaining the same number of images per class as before. The re-trained model not only generalizes better (90% test accuracy), but also looks at the right regions (last column of Fig. 8). This experiment demonstrates a proof-of-concept that Grad-CAM can help detect and remove biases in datasets, which is im-portant not just for better generalization, but also for fair and ethical outcomes as more algorithmic decisions are made in society.

Textual Explanations with Grad-CAM

Equation. (1) gives a way to obtain neuron-importance, α, for each neuron in a convolutional layer for a particular class. There have been hypotheses presented in the literature [60, 57] that neurons act as concept ‘detectors’. Higher positive values of the neuron importance indicate that the presence of that concept leads to an increase in the class score, whereas higher negative values indicate that its absence leads to an increase in the score for the class.
Given this intuition, let’s examine a way to generate tex-tual explanations. In recent work, Bau et al. [4] proposed
an approach to automatically name neurons in any convo-lutional layer of a trained network. These names indicate concepts that the neuron looks for in an image. Using their approach. we first obtain neuron names for the last convolu-tional layer. Next, we sort and obtain the top-5 and bottom-5 neurons based on their class-specific importance scores, αk. The names for these neurons can be used as text explanations.
Fig. 9 shows some examples of visual and textual explana-tions for the image classification model (VGG-16) trained on the Places365 dataset [61]. In (a), the positively important neurons computed by (1) look for intuitive concepts such as book and shelf that are indicative of the class ‘Book-store’. Also note that the negatively important neurons look for con-cepts such as sky, road, water and car which don’t occur in ‘Book-store’ images. In (b), for predicting ‘waterfall’, both visual and textual explanations highlight ‘water’ and ‘strat-ified’ which are descriptive of ‘waterfall’ images. (e) is a failure case due to misclassification as the network predicted ‘rope-bridge’ when there is no rope, but still the important concepts (water and bridge) are indicative of the predicted class. In (f), while Grad-CAM correctly looks at the door and the staircase on the paper to predict ‘Elevator door’, the neurons detecting doors did not pass the IoU threshold7 of
0.05 (chosen in order to suppress the noise in the neuron names), and hence are not part of the textual explanations. More qualitative examples can be found in the Sec. F.

Grad-CAM for Image Captioning and VQA

Finally, we apply Grad-CAM to vision & language tasks such as image captioning [7, 29, 55] and Visual Question Answer-ing (VQA) [3, 20, 42, 46]. We find that Grad-CAM leads to interpretable visual explanations for these tasks as compared to baseline visualizations which do not change noticeably across changing predictions. Note that existing visualization techniques either are not class-discriminative (Guided Back-propagation, Deconvolution), or simply cannot be used for these tasks/architectures, or both (CAM, c-MWP).

Image Captioning

In this section, we visualize spatial support for an image captioning model using Grad-CAM. We build Grad-CAM on top of the publicly available neuraltalk28 implementa-tion [31] that uses a finetuned VGG-16 CNN for images and an LSTM-based language model. Note that this model does not have an explicit attention mechanism. Given a caption, we compute the gradient of its log probability w.r.t. units in

7 Area of overlap between ground truth concept annotation and neu-ron activation over area of their union. More details of this metric can be found in [4]
8    https://github.com/karpathy/neuraltalk2

	
(a)	(b)

(c)	(d)

(e)	(f)
Fig. 9: Examples showing visual explanations and textual explanations for VGG-16 trained on Places365 dataset [61]. For textual explanations we provide the most important neurons for the predicted class along with their names. Important neurons can be either be persuasive (positive importance) or inhibitive (negative importance). The first 2 rows show success cases, and the last row shows 2 failure cases. We see that in (a), the important neurons computed by (1) look for concepts such as book and shelf which are indicative of class ‘Book-store’ which is fairly intuitive.

(a) Image captioning explanations	(b) Comparison to DenseCap
Fig. 10: Interpreting image captioning models: We use our class-discriminative localization technique, Grad-CAM to find spatial support regions for captions in images. Fig. 10a Visual explanations from image captioning model [31] highlighting image regions considered to be important for producing the captions. Fig. 10b Grad-CAM localizations of a global or holistic captioning model for captions generated by a dense captioning model [29] for the three bounding box proposals marked on the left. We can see that we get back Grad-CAM localizations (right) that agree with those bounding boxes – even though the captioning model and Grad-CAM techniques do not use any bounding box annotations.

the last convolutional layer of the CNN (conv5_3 for VGG-16) and generate Grad-CAM visualizations as described in Sec. 3. See Fig. 10a. In the first example, Grad-CAM maps for the generated caption localize every occurrence of both the kites and people despite their relatively small size. In the next example, Grad-CAM correctly highlights the pizza and the man, but ignores the woman nearby, since ‘woman’ is not mentioned in the caption. More examples are in Sec. C.
Comparison to dense captioning. Johnson et al. [29] re-cently introduced the Dense Captioning (DenseCap) task that requires a system to jointly localize and caption salient regions in a given image. Their model consists of a Fully Convolutional Localization Network (FCLN) that produces bounding boxes for regions of interest and an LSTM-based language model that generates associated captions, all in a single forward pass. Using DenseCap, we generate 5 region-
specific captions per image with associated ground truth bounding boxes. Grad-CAM for a whole-image captioning model (neuraltalk2) should localize the bounding box the region-caption was generated for, which is shown in Fig. 10b. We quantify this by computing the ratio of mean activation in-side vs. outside the box. Higher ratios are better because they indicate stronger attention to the region the caption was gen-erated for. Uniformly highlighting the whole image results in a baseline ratio of 1.0 whereas Grad-CAM achieves 3.27
± 0.18. Adding high-resolution detail gives an improved baseline of 2.32 ± 0.08 (Guided Backpropagation) and the best localization at 6.38 ± 0.99 (Guided Grad-CAM). Thus, Grad-CAM is able to localize regions in the image that the DenseCap model describes, even though the holistic caption-ing model was never trained with bounding-box annotations.

	
(a)	(b)
Fig. 11: Qualitative Results for our word-level captioning experiments: (a) Given the image on the left and the caption, we visualize Grad-CAM maps for the visual words “bike", “bench" and “bus". Note how well the Grad-CAM maps correlate with the COCO segmentation maps on the right column. (b) shows a similar example where we visualize Grad-CAM maps for the visual words “people", “bicycle" and “bird".

Grad-CAM for individual words of caption

In our experiment we use the Show and Tell model [55] pre-trained on MSCOCO without fine-tuning through the visual representation obtained from Inception [54] architecture. In order to obtain Grad-CAM map for individual words in the ground-truth caption we one-hot encode each of the visual words at the corresponding time-steps and compute the neu-ron importance score using Eq. (1) and combine with the convolution feature maps using Eq. (2).
Comparison to Human Attention We manually created an object category to word mapping that maps object categories like <person> to a list of potential fine-grained labels like [“child”, “man”, "woman", ...]. We map a total of 830 visual words existing in COCO captions to 80 COCO categories. We then use the segmentation annotations for the 80 categories as human attention for this subset of matching words.
We then use the pointing evaluation from [58]. For each visual word from the caption, we generate the Grad-CAM map and then extract the maximally activated point. We then evaluate if the point lies within the human attention mapseg-mentation for the corresponding COCO category, thereby counting it as a hit or a miss. The pointing accuracy is then calculated as
Acc =   #Hits   . We perform this experiment on 1000 randomly sampled images from COCO dataset and ob-tain an accuracy of 30.0%. Some qualitative examples can be found in Fig. 11.
Visual Question Answering

Typical VQA pipelines [3, 20, 42, 46] consist of a CNN to process images and an RNN language model for questions. The image and the question representations are fused to pre-dict the answer, typically with a 1000-way classification (1000 being the size of the answer space). Since this is a classification problem, we pick an answer (the score yc in
(3)) and use its score to compute Grad-CAM visualizations over the image to explain the answer. Despite the complexity of the task, involving both visual and textual components, the explanations (of the VQA model from Lu et al. [38]) de-scribed in Fig. 12 are surprisingly intuitive and informative. We quantify the performance of Grad-CAM via correlation with occlusion maps, as in Sec. 5.3. Grad-CAM achieves a rank correlation (with occlusion maps) of 0.60 ± 0.038 whereas Guided Backpropagation achieves 0.42 ± 0.038, in-dicating higher faithfulness of our Grad-CAM visualization.
Comparison to Human Attention. Das et al. [9] collected human attention maps for a subset of the VQA dataset [3]. These maps have high intensity where humans looked in the image in order to answer a visual question. Human attention maps are compared to Grad-CAM visualizations for the VQA model from [38] on 1374 val question-image (QI) pairs from
[3] using the rank correlation evaluation protocol as in [9]. Grad-CAM and human attention maps have a correlation of 0.136, which is higher than chance or random attention maps (zero correlation). This shows that despite not being trained on grounded image-text pairs, even non-attention



Visualizing VQA model from [38]


Visualizing ResNet based Hierarchical co-attention VQA model from [39]
Fig. 12: Qualitative Results for our VQA experiments: (a) Given the image on the left and the question “What color is the firehydrant?”, we visualize Grad-CAMs and Guided Grad-CAMs for the answers “red", “yellow" and “yellow and red". Grad-CAM visualizations are highly interpretable and help explain any target prediction – for “red”, the model focuses on the bottom red part of the firehydrant; when forced to answer “yellow”, the model concentrates on it‘s top yellow cap, and when forced to answer “yellow and red", it looks at the whole firehydrant! (b) Our approach is capable of providing interpretable explanations even for complex models.


based CNN + LSTM based VQA models are surprisingly good at localizing regions for predicting a particular answer.
Visualizing ResNet-based VQA model with co-attention. Lu et al. [39] use a 200 layer ResNet [24] to encode the image, and jointly learn a hierarchical attention mechanism on the question and image. Fig. 12b shows Grad-CAM visu-alizations for this network. As we visualize deeper layers of the ResNet, we see small changes in Grad-CAM for most ad-jacent layers and larger changes between layers that involve dimensionality reduction. More visualizations for ResNets can be found in Sec. G. To the best of our knowledge, we are the first to visualize decisions from ResNet-based models.
Conclusion

In this work, we proposed a novel class-discriminative lo-calization technique – Gradient-weighted Class Activation Mapping (Grad-CAM) – for making any CNN-based model more transparent by producing visual explanations. Further, we combined Grad-CAM localizations with existing high-resolution visualization techniques to obtain the best of both worlds – high-resolution and class-discriminative Guided Grad-CAM visualizations. Our visualizations outperform existing approaches on both axes – interpretability and faith-fulness to original model. Extensive human studies reveal that our visualizations can discriminate between classes more accurately, better expose the trustworthiness of a classifier, and help identify biases in datasets. Further, we devise a way to identify important neurons through Grad-CAM and pro-vide a way to obtain textual explanations for model decisions. Finally, we show the broad applicability of Grad-CAM to various off-the-shelf architectures for tasks such as image classification, image captioning and visual question answer-ing. We believe that a true AI system should not only be intelligent, but also be able to reason about its beliefs and actions for humans to trust and use it. Future work includes explaining decisions made by deep networks in domains such as reinforcement learning, natural language processing and video applications.

Acknowledgements

This work was funded in part by NSF CAREER awards to DB and DP, DARPA XAI grant to DB and DP, ONR YIP awards to DP and DB, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, an Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and DP, Google Faculty Research Awards to DP and DB, Amazon Academic Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.

Appendix
A Appendix Overview

In the appendix, we provide:
- Ablation studies evaluating our design choices
- More qualitative examples for image classification, captioning and VQA
- More details of Pointing Game evaluation technique
- Qualitative comparison to existing visualization tech-niques
- More qualitative examples of textual explanations

B Ablation studies

We perform several ablation studies to explore and validate our design choices for computing Grad-CAM visualizations. This includes visualizing different layers in the network, un-derstanding importance of ReLU in (2), analyzing different types of gradients (for ReLU backward pass), and different gradient pooling strategies.

Grad-CAM for different layers

We show Grad-CAM visualizations for the “tiger-cat” class at different convolutional layers in AlexNet and VGG-16. As expected, the results from Fig. 13 show that localization becomes progressively worse as we move to earlier convo-lutional layers. This is because later convolutional layers better capture high-level semantic information while retain-ing spatial information than earlier layers, that have smaller receptive fields and only focus on local features.

Design choices
(GMP). We observe that using GMP lowers the localization ability of Grad-CAM. An example can be found in Fig. 15 below. This may be due to the fact that max is statistically less robust to noise compared to the averaged gradient.

2.3. Effect of different ReLU on Grad-CAM

We experiment with Guided-ReLU [53] and Deconv-ReLU [57] as modifications to the backward pass of ReLU.
Guided-ReLU: Springenberg et al. [53] introduced Guided Backprop, where the backward pass of ReLU is modified to only pass positive gradients to regions of positive activa-tions. Applying this change to the computation of Grad-CAM introduces a drop in the class-discriminative ability as can be seen in Fig. 16, but it marginally improves localization performance as can be seen in Table. 3.
Deconv-ReLU: In Deconvolution [57], Zeiler and Fergus introduced a modification to the backward pass of ReLU to only pass positive gradients. Applying this modification to the computation of Grad-CAM leads to worse results (Fig. 16). This indicates that negative gradients also carry important information for class-discriminativeness.

C Qualitative results for vision and language tasks

In this section we provide more qualitative results for Grad-CAM and Guided Grad-CAM applied to the task of image
		classification, image captioning and VQA.








Table 3: Localization results on ILSVRC-15 val for the ablations. Note that this evaluation is over 10 crops, while visualizations are single crop.


We evaluate different design choices via top-1 localization errors on the ILSVRC-15 val set [14]. See Table. 3.

Importance of ReLU in (3)

Removing ReLU ((3)) increases error by 15.3%. Negative values in Grad-CAM indicate confusion between multiple occurring classes.

Global Average Pooling vs. Global Max Pooling

Instead of Global Average Pooling (GAP) the incoming gradi-ents to the convolutional layer, we tried Global Max Pooling
Image Classification

We use Grad-CAM and Guided Grad-CAM to visualize the regions of the image that provide support for a particular prediction. The results reported in Fig. 17 correspond to the VGG-16 [52] network trained on ImageNet.
Fig. 17 shows randomly sampled examples from COCO [35] validation set. COCO images typically have multiple objects per image and Grad-CAM visualizations show precise local-ization to support the model’s prediction.
Guided Grad-CAM can even localize tiny objects. For ex-ample our approach correctly localizes the predicted class “torch” (Fig. 17.a) inspite of its size and odd location in the image. Our method is also class-discriminative – it places attention only on the “toilet seat” even when a popular Ima-geNet category “dog” exists in the image (Fig. 17.e).
We also visualized Grad-CAM, Guided Backpropagation (GB), Deconvolution (DC), GB + Grad-CAM (Guided Grad-CAM), DC + Grad-CAM (Deconvolution Grad-CAM) for images from the ILSVRC13 detection val set that have at least 2 unique object categories each. The visualizations for the mentioned class can be found in the following links.


Fig. 13: Grad-CAM at different convolutional layers for the ‘tiger cat’ class. This figure analyzes how localizations change qualitatively as we perform Grad-CAM with respect to different feature maps in a CNN (VGG16 [52]). We find that the best looking visualizations are often obtained after the deepest convolutional layer in the network, and localizations get progressively worse at shallower layers. This is consistent with our intuition described in Section 3 of main paper, that deeper convolutional layer capture more semantic concepts.
Fig. 14: Grad-CAM localizations for “tiger cat” category for different rectified convolutional layer feature maps for AlexNet.

Image Captioning








Fig. 15: Grad-CAM visualizations for “tiger cat” category with Global Average Pooling and Global Max Pooling.


Fig. 16: Grad-CAM visualizations for “tiger cat” category for different modifications to the ReLU backward pass. The best results are obtained when we use the actual gradients during the computation of Grad-CAM.




“computer keyboard, keypad” class: http://i.imgur.com/QMhsRzf.jpg
“sunglasses, dark glasses, shades” class: http://i.imgur.com/a1C7DGh.jpg
We use the publicly available Neuraltalk2 code and model9 for our image captioning experiments. The model uses VGG-16 to encode the image. The image representation is passed as input at the first time step to an LSTM that generates a caption for the image. The model is trained end-to-end along with CNN finetuning using the COCO [35] Captioning dataset. We feedforward the image to the image captioning model to obtain a caption. We use Grad-CAM to get a coarse localization and combine it with Guided Backpropagation to get a high-resolution visualization that highlights regions in the image that provide support for the generated caption.


Visual Question Answering (VQA)

We use Grad-CAM and Guided Grad-CAM to explain why a publicly available VQA model [38] answered what it an-swered.
The VQA model by Lu et al. uses a standard CNN followed by a fully connected layer to transform the image to 1024-dim to match the LSTM embeddings of the question. Then the transformed image and LSTM embeddings are pointwise

9    https://github.com/karpathy/neuraltalk2



Fig. 17: Visualizations for randomly sampled images from the COCO validation dataset. Predicted classes are mentioned at the top of each column.

multiplied to get a combined representation of the image and question and a multi-layer perceptron is trained on top to pre-dict one among 1000 answers. We show visualizations for the VQA model trained with 3 different CNNs - AlexNet [33], VGG-16 and VGG-19 [52]. Even though the CNNs were not finetuned for the task of VQA, it is interesting to see how our approach can serve as a tool to understand these networks bet-ter by providing a localized high-resolution visualization of the regions the model is looking at. Note that these networks were trained with no explicit attention mechanism enforced.
Notice in the first row of Fig. 19, for the question, “Is the person riding the waves?”, the VQA model with AlexNet and VGG-16 answered “No”, as they concentrated on the person mainly, and not the waves. On the other hand, VGG-19 correctly answered “Yes”, and it looked at the regions around the man in order to answer the question. In the second row, for the question, “What is the person hitting?”, the VQA model trained with AlexNet answered “Tennis ball” just based on context without looking at the ball. Such a model might be risky when employed in real-life scenarios. It is difficult to determine the trustworthiness of a model just based on the predicted answer. Our visualizations provide an accurate way to explain the model’s predictions and help
in determining which model to trust, without making any architectural changes or sacrificing accuracy. Notice in the last row of Fig. 19, for the question, “Is this a whole orange?”, the model looks for regions around the orange to answer “No”.


D More details of Pointing Game

In [58], the pointing game was setup to evaluate the dis-criminativeness of different attention maps for localizing ground-truth categories. In a sense, this evaluates the preci-sion of a visualization, i.e. how often does the attention map intersect the segmentation map of the ground-truth category. This does not evaluate how often the visualization technique produces maps which do not correspond to the category of interest.
Hence we propose a modification to the pointing game to evaluate visualizations of the top-5 predicted category. In this case the visualizations are given an additional option to reject any of the top-5 predictions from the CNN classifiers. For each of the two visualizations, Grad-CAM and c-MWP, we choose a threshold on the max value of the visualization,


Fig. 18: Guided Backpropagation, Grad-CAM and Guided Grad-CAM visualizations for the captions produced by the Neuraltalk2 image captioning model.


Fig. 19: Guided Backpropagation, Grad-CAM and Guided Grad-CAM visualizations for the answers from a VQA model. For each image-question pair, we show visualizations for AlexNet, VGG-16 and VGG-19. Notice how the attention changes in row 3, as we change the answer from Yellow to Green.


Fig. 20: Visualizations for ground-truth categories (shown below each image) for images sampled from the PASCAL [17] validation set.

	

Fig. 21: More Qualitative examples showing visual explanations and textual explanations for VGG-16 trained on Places365 dataset ([61]). For textual explanations we provide the most important neurons for the predicted class along with their names. Important neurons can be either be persuasive (positively important) or inhibitive (negatively important). The first 3 rows show positive examples, and the last 2 rows show failure cases.

that can be used to determine if the category being visualized exists in the image.
We compute the maps for the top-5 categories, and based on the maximum value in the map, we try to classify if the map is of the GT label or a category that is absent in the image. As mentioned in Section 4.2 of the main paper, we find that our approach Grad-CAM outperforms c-MWP by a significant margin (70.58% vs 60.30% on VGG-16).

E Qualitative comparison to Excitation Backprop (c-MWP) and CAM
In this section we provide more qualitative results comparing Grad-CAM with CAM [59] and c-MWP [58] on Pascal [17].
We compare Grad-CAM, CAM and c-MWP visualizations from ImageNet trained VGG-16 models finetuned on PAS-CAL VOC 2012 dataset. While Grad-CAM and c-MWP visualizations can be directly obtained from existing mod-els, CAM requires an architectural change, and requires re-training, which leads to loss in accuracy. Also, unlike Grad-CAM, c-MWP and CAM can only be applied for image classification networks. Visualizations for the ground-truth categories can be found in Fig. 20.

F Visual and Textual explanations for Places dataset Fig. 21 shows more examples of visual and textual explana-tions (Sec. 7) for the image classification model (VGG-16) trained on Places365 dataset ([61]).


(a) Grad-CAM visualizations for the ResNet-200 layer architecture for (b) Grad-CAM visualizations for the ResNet-200 layer architecture for
’tiger cat’(left) and ’boxer’(right) category.	’tabby cat’(left) and ’boxer’(right) category.

Fig. 22: We observe that the discriminative ability of Grad-CAM significantly reduces as we encounter the downsampling layer.

G Analyzing Residual Networks
In this section, we perform Grad-CAM on Residual Networks (ResNets). In particular, we analyze the 200-layer architec-ture trained on ImageNet10.
Current ResNets [24] typically consist of residual blocks. One set of blocks use identity skip connections (shortcut connections between two layers having identical output di-

10 We use the 200-layer ResNet architecture from https:// github.com/facebook/fb.resnet.torch.
mensions). These sets of residual blocks are interspersed with downsampling modules that alter dimensions of propagating signal. As can be seen in Fig. 22 our visualizations applied on the last convolutional layer can correctly localize the cat and the dog. Grad-CAM can also visualize the cat and dog cor-rectly in the residual blocks of the last set. However, as we go towards earlier sets of residual blocks with different spatial resolution, we see that Grad-CAM fails to localize the cate-gory of interest (see last row of Fig. 22). We observe similar trends for other ResNet architectures (18 and 50-layer).